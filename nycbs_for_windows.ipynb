{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Acquisition & Preparation\n",
    "\n",
    "## a. Downloading the Data:\n",
    "The latest monthly Citi Bike dataset is available from the Citi Bike System Data S3 bucket (e.g., https://s3.amazonaws.com/tripdata/index.html). Everymonth two new files are added:\n",
    "1. For Jersey City -> JC-YYYYMM-citibike-tripdata.csv.zip\n",
    "2. For NYC -> YYYYMM-citibike-tripdata.zip (From 2013 till 2023 files are anually, from 2024 upwards files are monthly)\n",
    "\n",
    "In case of January 2025 there is three files availble in the zip (_1,_2,_3) sharing same schema\n",
    "\n",
    "## b. Ingesting the Data with PySpark:\n",
    "Start a Spark session and read the CSV file into a DataFrame. We can let Spark infer the schema (or provide an explicit schema for better performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted c:\\Git\\NYCBS\\raw_data\\202501-citibike-tripdata.zip to c:\\Git\\NYCBS\\data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import platform\n",
    "\n",
    "def extract_all_zips(zip_folder, extract_to):\n",
    "    \"\"\"\n",
    "    Extracts all zip files found in the given folder into the specified extraction directory.\n",
    "\n",
    "    Parameters:\n",
    "    zip_folder (str): Path to the folder containing zip files.\n",
    "    extract_to (str): Path to the directory where the CSV files will be extracted.\n",
    "    \"\"\"\n",
    "    # Normalize paths for cross-platform compatibility\n",
    "    zip_folder = os.path.abspath(zip_folder)\n",
    "    extract_to = os.path.abspath(extract_to)\n",
    "\n",
    "    # Ensure the extraction directory exists.\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    # Find all zip files in the specified folder.\n",
    "    zip_files = glob.glob(os.path.join(zip_folder, \"*.zip\"))\n",
    "\n",
    "    # Iterate over each zip file and extract its contents.\n",
    "    for zip_file in zip_files:\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "            print(f\"Extracted {zip_file} to {extract_to}\")\n",
    "\n",
    "# --- Usage ---\n",
    "\n",
    "# Set paths for zip folder and extraction directory\n",
    "current_dir = os.getcwd()\n",
    "zip_folder_path = os.path.join(current_dir, \"raw_data\")  # Update as needed\n",
    "extract_dir = os.path.join(current_dir, \"data\")          # Folder to hold extracted CSV files\n",
    "\n",
    "# Ensure paths are formatted correctly for Spark (Windows needs forward slashes)\n",
    "if platform.system() == \"Windows\":\n",
    "    zip_folder_path = zip_folder_path.replace(os.sep, \"/\")\n",
    "    extract_dir = extract_dir.replace(os.sep, \"/\")\n",
    "\n",
    "# Extract all CSV files from the zip files.\n",
    "extract_all_zips(zip_folder_path, extract_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     31\u001b[39m uber_jar_path = uber_jar_path.replace(os.sep, \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Ensure forward slashes for Spark compatibility\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Initialize Spark\u001b[39;00m\n\u001b[32m     34\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSparkIcebergExample\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[1]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.jars\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muber_jar_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.catalog.local_pc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.iceberg.spark.SparkCatalog\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.catalog.local_pc.type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjdbc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.catalog.local_pc.uri\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.catalog.local_pc.jdbc.driver\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.sqlite.JDBC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.catalog.local_pc.warehouse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarehouse_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.catalogImplementation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43min-memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.catalog.local_pc.datanucleus.schema.autoCreateTables\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.legacy.createHiveTableByDefault.enabled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfalse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.hive.metastore.sharedPrefixes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.catalog.local_pc.jdbc.connection.url\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.extensions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark session created\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark version:\u001b[39m\u001b[33m\"\u001b[39m, spark.version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\NYCBS\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\NYCBS\\.venv\\Lib\\site-packages\\pyspark\\context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\NYCBS\\.venv\\Lib\\site-packages\\pyspark\\context.py:201\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    204\u001b[39m         master,\n\u001b[32m    205\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         memory_profiler_cls,\n\u001b[32m    216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\NYCBS\\.venv\\Lib\\site-packages\\pyspark\\context.py:436\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\NYCBS\\.venv\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    108\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m         message_parameters={},\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    113\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Ensure paths are correctly formatted for both Windows and Mac/Linux\n",
    "catalog_path = os.path.join(current_dir, \"catalog\", \"iceberg.db\")\n",
    "db_dir = os.path.dirname(catalog_path)\n",
    "os.makedirs(db_dir, exist_ok=True)\n",
    "\n",
    "# Handle JDBC URL formatting for different OS\n",
    "if platform.system() == \"Windows\":\n",
    "    jdbc_url = f\"jdbc:sqlite:///{catalog_path.replace(os.sep, '/')}\"  # Convert \\ to /\n",
    "    warehouse_url = f\"file:///{os.path.join(current_dir, 'dwh').replace(os.sep, '/')}\"\n",
    "else:\n",
    "    jdbc_url = f\"jdbc:sqlite:///{catalog_path}\"\n",
    "    warehouse_url = f\"file://{os.path.join(current_dir, 'dwh')}\"\n",
    "\n",
    "# Set the directory where CSV files have been extracted\n",
    "extract_dir = os.path.join(current_dir, \"data\")\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Ensure the \"dwh\" warehouse directory exists\n",
    "warehouse_dir = os.path.join(current_dir, \"dwh\")\n",
    "os.makedirs(warehouse_dir, exist_ok=True)\n",
    "\n",
    "# Path for the Iceberg helper JAR\n",
    "uber_jar_path = os.path.join(current_dir, \"spark-iceberg-helper\", \"target\", \"spark-iceberg-helper-1.0-SNAPSHOT.jar\")\n",
    "uber_jar_path = uber_jar_path.replace(os.sep, \"/\")  # Ensure forward slashes for Spark compatibility\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkIcebergExample\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.jars\", uber_jar_path) \\\n",
    "    .config(\"spark.sql.catalog.local_pc\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local_pc.type\", \"jdbc\") \\\n",
    "    .config(\"spark.sql.catalog.local_pc.uri\", jdbc_url) \\\n",
    "    .config(\"spark.sql.catalog.local_pc.jdbc.driver\", \"org.sqlite.JDBC\") \\\n",
    "    .config(\"spark.sql.catalog.local_pc.warehouse\", warehouse_url) \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"in-memory\") \\\n",
    "    .config(\"spark.sql.catalog.local_pc.datanucleus.schema.autoCreateTables\", \"true\") \\\n",
    "    .config(\"spark.sql.legacy.createHiveTableByDefault.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.hive.metastore.sharedPrefixes\", \"\") \\\n",
    "    .config(\"spark.sql.catalog.local_pc.jdbc.connection.url\", jdbc_url) \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Java version:\", spark._jvm.System.getProperty(\"java.version\"))\n",
    "\n",
    "# Print relevant Spark configurations\n",
    "for key, value in spark.sparkContext.getConf().getAll():\n",
    "    if \"catalog.local_pc\" in key:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Read and display the contents of the CSV files\n",
    "csv_path = os.path.join(extract_dir, \"*.csv\").replace(os.sep, \"/\")  # Ensure forward slashes for Spark compatibility\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "print(f\"Number of rows: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "db_dir = os.path.dirname(f\"{current_dir}/catalog/iceberg.db\")\n",
    "os.makedirs(db_dir, exist_ok=True)\n",
    "\n",
    "jdbc_url = f\"jdbc:sqlite:///{current_dir}/catalog/iceberg.db\"\n",
    "warehouse_url = f\"file://{current_dir}/dwh/\"\n",
    "\n",
    "# Set the directory where CSV files have been extracted.\n",
    "extract_dir = \"data\"  # Adjust as needed.\n",
    "if not os.path.exists(extract_dir):\n",
    "    os.makedirs(extract_dir)\n",
    "\n",
    "# Ensure the \"dwh\" warehouse directory exists.\n",
    "warehouse_dir = \"dwh\"\n",
    "if not os.path.exists(warehouse_dir):\n",
    "    os.makedirs(warehouse_dir)\n",
    "\n",
    "\n",
    "#  jar named spark-iceberg-helper-1.0-SNAPSHOT.jar downloaded via maven\n",
    "uber_jar_path = \"spark-iceberg-helper/target/spark-iceberg-helper-1.0-SNAPSHOT.jar\"\n",
    "\n",
    "# Initialize Spark\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"SparkIcebergExample\") \\\n",
    "#     .config(\"spark.jars\", uber_jar_path) \\\n",
    "#     .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "#     .config(\"spark.sql.catalog.local.type\", \"sql\") \\\n",
    "#     .config(\"spark.sql.catalog.local.warehouse\", \"file:///Users/aldam/git/NYCBS/dwh\") \\\n",
    "#     .config(\"spark.sql.catalog.local.uri\", \"sqlite:///Users/aldam/git/NYCBS/catalog/iceberg.db\") \\\n",
    "#     .config(\"spark.sql.catalog.local.datanucleus.schema.autoCreateTables\", \"true\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "#    .config(\"spark.sql.catalog.local.jdbc.connection.url\", jdbc_url) \\\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkIcebergExample\") \\\n",
    "    .config(\"spark.jars\", uber_jar_path) \\\n",
    "    .config(\"spark.sql.catalog.local_pc\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local_pc.type\", \"jdbc\") \\\n",
    "    .config(\"spark.sql.catalog.local_pc.uri\", jdbc_url) \\\n",
    "    .config(\"spark.sql.catalog.local_pc.jdbc.driver\", \"org.sqlite.JDBC\") \\\n",
    "    .config(\"spark.sql.catalog.local_pc.warehouse\", warehouse_url) \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"in-memory\") \\\n",
    "    .config(\"spark.sql.catalog.local_pc.datanucleus.schema.autoCreateTables\", \"true\") \\\n",
    "    .config(\"spark.sql.legacy.createHiveTableByDefault.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.hive.metastore.sharedPrefixes\", \"\") \\\n",
    "    .config(\"spark.sql.catalog.local_pc.jdbc.connection.url\", jdbc_url) \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"SparkIcebergExample\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "for key, value in spark.sparkContext.getConf().getAll():\n",
    "    if \"catalog.local_pc\" in key:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Read and display the contents of the CSV files.\n",
    "df = spark.read.csv(os.path.join(extract_dir, \"*.csv\"), header=True, inferSchema=True)\n",
    "df.show()\n",
    "print(f\"Number of rows: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Iceberg\n",
    "Create a new catalog and a test table using Iceberg and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema, NestedField, IntegerType, StringType\n",
    "\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "db_path = os.path.join(current_dir, \"catalog\", \"iceberg.db\")\n",
    "warehouse_path = os.path.join(current_dir, \"dwh\")\n",
    "tables_path = os.path.join(current_dir, \"tables\", \"example_table\")\n",
    "\n",
    "# Make sure directories exist\n",
    "os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "os.makedirs(warehouse_path, exist_ok=True)\n",
    "os.makedirs(tables_path, exist_ok=True)\n",
    "\n",
    "# Explicitly format the JDBC URL with 4 slashes, since that's what works\n",
    "jdbc_url = f\"sqlite:///{db_path}\"\n",
    "warehouse_url = f\"file://{warehouse_path}\"\n",
    "tables_url = f\"file://{tables_path}\"\n",
    "\n",
    "print(f\"jdbc_url: {jdbc_url}\")\n",
    "print(f\"warehouse_url: {warehouse_url}\")\n",
    "print(f\"tables_url: {tables_url}\")\n",
    "\n",
    "# Set the directory where CSV files have been extracted.\n",
    "catalog_dir = \"catalog\"  # Adjust as needed.\n",
    "if not os.path.exists(catalog_dir):\n",
    "    os.makedirs(catalog_dir)\n",
    "\n",
    "\n",
    "# Create a sql catalog using a local file system.\n",
    "catalog = load_catalog(\n",
    "    name=\"local_pc\",\n",
    "    uri=jdbc_url,\n",
    "    type=\"sql\",\n",
    "    warehouse=warehouse_url\n",
    ")\n",
    "\n",
    "\n",
    "# Create the metastore tables if they don't exist.\n",
    "catalog.create_tables()\n",
    "\n",
    "# (Optional) Create a namespace if needed.\n",
    "catalog.create_namespace_if_not_exists(\"default\")\n",
    "\n",
    "# Define the table schema using the Schema API.\n",
    "schema = Schema(\n",
    "    NestedField(field_id=1, name=\"id\", field_type=IntegerType(), required=True),\n",
    "    NestedField(field_id=2, name=\"data\", field_type=StringType(), required=True)\n",
    ")\n",
    "\n",
    "# Create the Iceberg table within the 'default' namespace.\n",
    "catalog.create_table_if_not_exists(\n",
    "    identifier=\"default.example_table\",  # qualified as namespace.table\n",
    "    schema=schema,\n",
    "    location=\"file:///Users/aldam/git/NYCBS/tables/example_table/\"\n",
    ")\n",
    "\n",
    "print(\"Catalog and table created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Java version:\", spark._jvm.System.getProperty(\"java.version\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in spark.sparkContext.getConf().getAll():\n",
    "    if \"catalog.local_pc\" in key:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "#spark.sql(\"USE CATALOG LOCAL_PC\")\n",
    "spark.sql(\"ALTER SESSION SET CURRENT_CATALOG = local_pc\")\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS default\")\n",
    "\n",
    "# Write DataFrame to Iceberg\n",
    "df.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"local_pc.default.example_table_df\")\n",
    "\n",
    "# Read DataFrame from Iceberg\n",
    "df_read = spark.read \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .load(\"local_pc.default.example_table_df\")\n",
    "\n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database in the Iceberg catalog if it doesn't exist\n",
    "#spark.sql(\"CREATE DATABASE IF NOT EXISTS iceberg_catalog.db\")\n",
    "\n",
    "# Create the namespace if it does not exist.\n",
    "#spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg_catalog.db\")\n",
    "# Switch to the Iceberg catalog's default namespace.\n",
    "#spark.sql(\"USE iceberg_catalog.db\")\n",
    "    \n",
    "# Optionally drop the table if it exists.\n",
    "#tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "#if any(row.tableName == \"my_iceberg_table\" for row in tables):\n",
    "#    spark.sql(\"DROP TABLE my_iceberg_table\")\n",
    "    \n",
    "#Write the DataFrame to an Iceberg table.\n",
    "#data.writeTo(\"iceberg_catalog.default.my_iceberg_table\") \\\n",
    "#    .create() \n",
    "\n",
    "    \n",
    "#print(\"Data successfully written to Iceberg table: iceberg_catalog.default.my_iceberg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the df schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe the df\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Data Cleaning:\n",
    "Perform cleaning tasks such as:\n",
    "-\tDropping rows with missing critical fields (e.g., start/end timestamps).\n",
    "-\tConverting timestamp strings to proper timestamp types.\n",
    "-\tFiltering out trips with negative or zero durations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analysis & Insights\n",
    "\n",
    "## A. Insurance Coverage Analysis\n",
    "\n",
    "Counting Trips Over 30 Minutes:\n",
    "- Filter the DataFrame for trips where trip_duration > 30 minutes and count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, to_date\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "insurance_trip_duration = 30\n",
    "\n",
    "# Drop rows with missing timestamps\n",
    "df = df.dropna(subset=[\"started_at\", \"ended_at\"])\n",
    "\n",
    "# extract rental_date from started_at\n",
    "df = df.withColumn(\"rental_date\", to_date(col(\"started_at\")))\n",
    "\n",
    "# Convert time columns to timestamp and compute trip duration in minutes\n",
    "df = df.withColumn(\"start_ts\", unix_timestamp(\"started_at\")) \\\n",
    "       .withColumn(\"end_ts\", unix_timestamp(\"ended_at\"))\n",
    "\n",
    "# Calculate duration (in minutes) and filter out invalid records\n",
    "df = df.withColumn(\"trip_duration\", (col(\"end_ts\") - col(\"start_ts\")) / 60)\n",
    "df = df.filter(col(\"trip_duration\") > 0)\n",
    "\n",
    "#add a column to flag trips that are longer than 30 minutes\n",
    "df = df.withColumn(\"insurance_trip\", F.when(col(\"trip_duration\") > insurance_trip_duration, 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_trips = df.filter(col(\"trip_duration\") > 30)\n",
    "num_insurance_trips = insurance_trips.count()\n",
    "print(f\"Trips longer than 30 minutes: {num_insurance_trips}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Revenue Estimation\n",
    "\n",
    "Estimating Revenue Impact:\n",
    "- Assume a charge of $0.20 per trip that exceeds 30 minutes. Multiply the count by $0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue = num_insurance_trips * 0.20\n",
    "print(f\"Estimated revenue from over-30-min rides: ${revenue:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Travel Distance Analysis\n",
    "\n",
    "1. Calculating the Distance (Haversine Formula):\n",
    "- As the dataset includes starting and ending coordinates (e.g., start station latitude, start station longitude, end station latitude, end station longitude), we can compute the great-circle distance using the haversine formula. \n",
    "- Hereâ€™s an example using Spark SQL functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Earth's radius in kilometers\n",
    "EARTH_RADIUS = 6371.0\n",
    "\n",
    "# Convert coordinates and calculate haversine 'a' factor\n",
    "df = df.withColumn(\"start_lat_rad\", F.radians(col(\"start_lat\"))) \\\n",
    "       .withColumn(\"start_lon_rad\", F.radians(col(\"start_lng\"))) \\\n",
    "       .withColumn(\"end_lat_rad\", F.radians(col(\"end_lat\"))) \\\n",
    "       .withColumn(\"end_lon_rad\", F.radians(col(\"end_lng\")))\n",
    "\n",
    "df = df.withColumn(\"dlat\", col(\"end_lat_rad\") - col(\"start_lat_rad\")) \\\n",
    "       .withColumn(\"dlon\", col(\"end_lon_rad\") - col(\"start_lon_rad\"))\n",
    "\n",
    "# Compute haversine distance in km\n",
    "df = df.withColumn(\"a\", F.pow(F.sin(col(\"dlat\")/2), 2) + \n",
    "                   F.cos(col(\"start_lat_rad\")) * F.cos(col(\"end_lat_rad\")) * \n",
    "                   F.pow(F.sin(col(\"dlon\")/2), 2))\n",
    "df = df.withColumn(\"distance_km\", 2 * EARTH_RADIUS * F.asin(F.sqrt(col(\"a\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Classifying Trips into Distance Buckets:\n",
    "Create a new column to assign each trip into one of the specified buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"distance_bucket\",\n",
    "    F.when(col(\"distance_km\") <= 1, \"0-1 km\")\n",
    "     .when((col(\"distance_km\") > 1) & (col(\"distance_km\") <= 4), \"2-4 km\")\n",
    "     .when((col(\"distance_km\") > 4) & (col(\"distance_km\") <= 9), \"4-9 km\")\n",
    "     .otherwise(\"10+ km\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualization:\n",
    "Convert the aggregated data (e.g., count per distance bucket) to Pandas and use matplotlib or seaborn to plot the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate counts by bucket and convert to Pandas DataFrame\n",
    "bucket_distribution = df.groupBy(\"distance_bucket\").count().toPandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.barplot(\n",
    "    x=\"distance_bucket\",\n",
    "    y=\"count\",\n",
    "    data=bucket_distribution,\n",
    "    palette=\"viridis\",\n",
    "    order=[\"0-1 km\", \"2-4 km\", \"4-9 km\", \"10+ km\"]  # Specify your custom order here\n",
    "    )\n",
    "ax.set_title(\"Trip Distance Distribution\")\n",
    "ax.set_xlabel(\"Distance Bucket\")\n",
    "ax.set_ylabel(\"Number of Trips\")\n",
    "\n",
    "# Example: set y-ticks at 0, 500k, 1M, 1.5M, etc.\n",
    "ax.set_yticks([0, 500_000, 1_000_000, 1_500_000])\n",
    "ax.set_yticklabels([\"0\", \"500K\", \"1M\", \"1.5M\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
